# sync data to cloud storage
5 * * * * gzip -9 /home/archiver/githubarchive/crawler/data/*.json && /usr/bin/gsutil cp -a public-read /home/archiver/githubarchive/crawler/data/`date +"\%Y-\%m-\%d-\%-k" -d "1 hour ago"`.json.gz gs://data.githubarchive.org
8 * * * * /usr/local/bin/s3cmd -c /home/archiver/.s3cfg --acl-public sync /home/archiver/githubarchive/crawler/data/*.gz s3://data.githubarchive.org/

# import data into bigquery
15 * * * * /bin/bash -l -c 'wget -nv -P /tmp http://data.githubarchive.org/`date +"\%Y-\%m-\%d-\%-k" -d "1 hour ago"`.json.gz && cd /home/archiver/githubarchive/bigquery && ruby sync.rb -f /tmp/`date +"\%Y-\%m-\%d-\%-k" -d "1 hour ago"`.json.gz --no-sync' >> /var/log/bigquery.log 2>&1

# run daily GH report
0 20 * * * /bin/bash -l -c 'cd /home/archiver/githubarchive/reports && HOMINID_KEY= HOMINID_LIST= ruby daily.rb'

# keep last 30 days worth of data
0 0 * * * find /home/archiver/githubarchive/crawler/data/* -mtime +30 -exec rm {} \;
